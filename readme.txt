1. files copied from "frequencydiagramsqmx" with on momentum analysis
	1.tpsa map generation
	see "compareTpsaWithMadxmap.py" to see how to generate tpsa from lte file.

	2.lte2tpsa.py
	from let generate input file as tpsa input (type *.py) to generate square matrix input file (type *.dat)

	3.ltecb2tpsa.py 
	same but cb2 lattice let file format differs from others

	4. lte2madx.py 
	from lte file generate input file for madx (type *.madx)

	5. ./madx nsls2sr_supercell_ch77_20160525.madx 
	run made to generate fort.18 and parameters_input for ipsa and twist

	6. compareTpsaWithMadxmap.py
	read madx (*.madx) or tpsa input (*.py) to generate square matrix (*.dat) as comparison

	7.jf51new.py from sqmxnsls (*dat) calculate tune and compare with tracking.

	8.tune.py  used to calculated tune and compare with tracking in jf51new

	9. generate lt5new.py in "sqmxnsls" and copy here to study method to get fluctuation
	more accurate than diffusion term (the coefficients of n*(n-1)) in the exponent

2. of momentum analysis
	1. jf1map_YH20190814.py
	2. jf1map_tpsa_delta_YH_20190823.py Yoshi compare mvp tpsa with his tpsa from lte file, with delda allowed, but I did not test it yet.
	3. jf1fastmap.py  Yoshi modified to generate tpsa map from lte without mvp tpsa
	4. jfmadx1mapM.py my tpsa with madx 
	5. jfdelta1Pmadx1mapM.py my madx tpsa with delta allowed for one values of delta P which may not be zero.
		there are comments on how to calculate off-momentum tpsa.
	6. improve lt5new.py, make it works in python3 environment, use it with jfM.dat generated by jfdeltaPmadx1mapM.py
		in order to check if without iteration, the larger fluctuation of wx and wy can represent approaching 
		stability border
	7. lt6.py: copy lt5new as lt6.py to revise so deltap can be non-zero.
		extensive test recorded in "tpsaFrequencyMap.pptx" with deltap=0.005 comparing x0d with tracking
	8. jfdelta2.py: 
		8.1 copied from jf56new.py to modify to allow for simple map with deltap nonzero.
		8.2 delete from jf56new submodules which are considered not usefull for now.
	9. jfdelta3.py
		9.1 combine jfdelta2.py with jfdelta1Pmadx1mapM.py so the tpsa map generated by madx
		and converted into square matrix is directly used for fluctuation map.
		9.2 allow for a list of deltap
	10 jfdelta4.py
 		mainly due to an error in scanx, where the starting point is |wx(x0,y0)| instead of wx(x0,y0) itself.
		Thus when x0 increases, the phase of wy(x0,y0) is very large and neglected by |wy(x0,y0)|, causing wx(x0,y0) blowup
		as xmax increases.
		10.2 jfdelta4 is used in moga by Yoshi till about 12/18/2019 and found optimization threshold too sensitive to
		very small resonance line, outside the line, jfdelta4 takes as large fluctuation even though motion is stable.
		so we need improve code based on recent success of 1d22henon.py in henonmap folder.
	11. 11.1 lt7.py, copy from lt6.py and revise following the steps in 1dhenon21.py, iterModules.py, and vcentralDisk.py,  
		to find exact solution by iteration. 
		11.2 modify 1d21henon in 'henonmap' steps into lt7.py, 
		11.2 copy iterModules.py as iter2DModules.py, parts from lt6.py
		11.3 copy vcentralDisk.py in henonmap as veqcb2delta005.py, part from lt6.py
		11.4 Found the bnm2 spectrum tracking does not agree with tracking spectrum very well i sqn vph.plotvnv0v1
			so decide to change compare with lt5.py of ../sqmxnsls for xmax=24e-3,ymax=4e-3 case where the spectrum
			agrees with tracking very well, even though it uses linear combination arinew.
	12.	
		12.1 lt8.py copy from lt7.py with veqcb2delta005.py replaced by veqnsls2.py
		12.2 revise ltecb2madx_case5.py into ltensls2sr_supercell_ch77_20150406_1madx_case5.py to be able 
			to read nsls2sr_supercell_ch77_20150406_1.lte and generate nsls2sr_supercell_ch77_20150406_1.madx file.
		12.3 revise jfdelta1Pmadx1mapM.py into nsls2sr_supercell_ch77_20150406_1madx1mapM.py to
			generate from nsls2sr_supercell_ch77_20150406_1.madx fort.18 file
		12.4 revise veqnsls2.py to using nsls2sr_supercell_ch77_20150406_1.lte to generate madx input file with deltap=0
		12.5 study case of xmax=22mm,ymax=4mm nsls2sr_supercell_ch77_20150406_1.lte, agree with lt5 of sqmxnsls
		12.6 slide 56, found the findso has error when it is not converging, solved by changing deltaxpt sign.
		12.7 slides 57-59 realized the importance and difficulty of Couchy cutoff when elegant only gives 7 digits precision.
		12.8 confirmed KAM iteration up to knew=2 but for xmax=22mm, there is resonance so when knew=4, there are spurious
			lines in bnm2new spectrum. In particular when thetadiv=80, spurious line is more serious.
		12.9 understand the difference between red tods and yellow dots of bnm2 spectrum, due to the approximation of exp(i*bnm2)
		12.10 realized the problem with wxp0, and v0norm, which makes changing linear combination arinew impossible when 
			changing initial value xmax to xmax=20mm
	
	lt9.py (recovered to lt9_differ_only_by_plot_in_iter2DModules.py because original itermodule file is lost.)
		1.copied from lt8.py trying to change  v0norm and wxp0, so they will not prevent new linear combination arinew
		2. after removing wxp0 and set v0norm=1, there is some improvement in the reduction of sputious lines
		3. When changing linear combination to xnew0 (simpleset linear combination) found problem in findso, it requires
			to increase the number of steps in findso. So revised to allow to increase nsostep in findso.
		4. But when move to xmax=20mm, it is much worse. Finaly realized xmax=20mm is indeed in stronger resonance than 22mm
			this is confirmed by checking the frequency diagram of sqmxnsls lt5.py slides.
		5. reduce xmax to xmax=18 mm solve the problem, get excellent agreement with tracking. Thus the method works.
		6. Confired why it does not work for xmax=19 or 20mm: stronger resonance than 22mm in slide 76
		7. decide to move forward to case of xmax=-1mm, ymax=6mm resonance (may be a smaller resonance than xmax=20mm)
		8. use veqnsls2sr_supercell_ch77_20150406_1_deltap0, not veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt8

	lt10.py 
		1. copy from lt9, study xmax=-1mm,ymax=6mm, starting from xmax=-0.98mm
		2. studied cases of xmax=-0.9mm, -0.95mm, all not as good as lt1 of sqmxnsls. trying to understand why and
			realized that uarray=[ux0,ux1,ux2,uy0] in lt1 of sqmxnsls, so go to lt11 try new way.
		
	lt11.py
		1. copy from lt10, change to uarray=[ux0,ux1,ux2,uy0], and realized in lt1 of sqmxnsls, it uses uarray=[ux0,ux1,uy0] first
			then uarray=[ux0,ux1,ux2,uy0] when at xmax=-0.9mm , this step greatly increased the precision of bnm1.
		2. trying to compare with lt1 of sqmxnsls for xmax=-0.8mm, with xari0, as the first step toward duplicate lt1 in lt11.
		3. modify veqnsls2sr_supercell_ch77_20150406_1_deltap0 to veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt11.py

	lt12.py
		1. copy from lt11.py, revise to allow iteration with changing xmax and arinow updated with arinew.
		2. scan xmax and plot rms(bnm) vs. xmax
	
	lt13.py
		1. copy from lt12, delete all after showarecord about KAM iteration, concentrating on ieration with arinew

	lte2deltapmadx2fort182mapMdat.py	
		1. madx map copied from jfdelta1Pmadx1mapM.py
		2. revised to give error messages when lte2madx
			or madx to fort.18 failed.
		3.goal:
			a. from lte file generate madx input using lte2..madx_case5.py file
			b. run ./madx ...madx to generate fort.18 file
			c. from fort.18 file gnerate jfM.dat file as input file to lt13.py
		4. this version use '20140204_bare_1supcell.lte' as example.

	lt14.py
		1. copy from lt13 to study 	cb2NSLS2CB65pm_cb0_1cell.lte at deltap=0 (?) between x=-3mm and x=-4mm, y=0, where sqmx gives wrong result
		2. change veqcb2NSLS2CB65pm_cb0_1cell from veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt11.py
		3. keep records about 
			3.1 change made in lt14 compare to lt13, and 
			3.2 compare veqcb2NSLS2CB65pm_cb0_1cell with veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt11.py
		4. indication that sqmx method cannot pass the strong resonance line of the type at x=-3mm, y=0.
	
	jfdelta4.py
		checked to apply to 20140204_bare_1supcell.lte and checked 20140204_bare_1supcell.dat and cb2NSLS2CB65pm_cb0_1cell_back.madx
		compared with Yoshi's data about over sensitive to resonance of jfdelta4 at deltap=-0.02

	lt15.py
		1. copied from lt14 to study 20140204_bare_1supcell.lte at deltap=-0.02 
		2. copy veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt11.py as veq20140204_bare_1supcell_deltapm02.py to be used in lt15.py
			to study 20140204_bare_1supcell at deltap=-0.02
		3. Fist tried to study veqcb2NSLS2CB65pm_cb0_1cell_deltaP0p005 as veq, to compare with lt6.py
			where deltap=0.005. saved as lt15_cb2deltaP0p005.py
		4. In modufying finso to revise  vthetascan_use_x0dlast as vthetascan_use_x0dlast1 to revise fsolve as fsolve1
			some errors were made, it took two days to solve the problem using lt15_veq20140204_bare_1supcell_deltapm02_x0y1em4.py
			where for the case xmax=0,ymax=1e-4, there were many wrong solutions for xyd0 for v20. after revision, the problem is solved
			in the exmaples of tmp(), tmp1(), runm02(), bnmnew1(), and vthetascan_use_x0dlast1 in iter2DModules.py (vph).

	lt16.py
		in study of lt15, found I lost tracking of data and I cannot reproduce slide 26 of madxFrequencyMap2.pptx for case x=-15mm,y=1e-4
			cp lt15_0.py  lt16.py
			!!! cp veq20140204_bare_1supcell_deltapm02.py  veq16.py. But veq16 uses rl("20140204_bare_1supcell.dat"),not rl("20140204_bare_1supcell_deltapm02.dat")!!! 
			cp junk.dir/iter2DModules_back.py  iter2DModules16.py (from junk.dir)
			Repeat case xmax,ymax=-0.015,1e-4
			the result seems identical to slide 26, the result is good convergence
			With small fluctuation, indicating the process is correct.
			uarray=np.array([ux0,ux1,ux2,uy0]), thetadiv=16
			arecord=runm012()
			showarecord(arecord[7])

	lt17
		1.cp  lt15_veq20140204_bare_1supcell_deltapm02_x0y1em4_8.py lt17.py and then revise
		2.cp veq20140204_bare_1supcell_deltapm02.py  veq17.py to study deltap=0,x=-15mm,y=1e-4, see if it is possible to reproduce slide lt16 or slide26
		3.rl("20140204_bare_1supcell.dat") in veq17, so deltap=0, keep uarray=np.array([ux0,ux1,ux2,uy0]) at first.
		4. change to xmax=-0.015, in runm012()
		5. keep using bnm() in runm012()
		6.cp iter2DModules.py iter2DModules17.py
			!!!But veq17 uses rl("20140204_bare_1supcell.dat"),not rl("20140204_bare_1supcell_deltapm02.dat")!!! 


	lt18
		1. cp veq17.py veq18.py revise to study deltap=-0.02,x=-15mm,y=1e-4, see if it is possible to reproduce slide 30-31
		2. cp iter2DModules17.py  iter2DModules18.py 
		3. cp lt17.py  lt18.py
		4. rl("20140204_bare_1supcell_deltapm02.dat") in veq18, so deltap=-0.02, keep uarray=np.array([ux0,ux1,ux2,uy0]) at first.
		5. change to xmax=-0.0, in runm012()
		6. keep using bnm() in runm012()
		7.cp iter2DModules.py iter2DModules18.py
		8. veq18 uses rl("20140204_bare_1supcell_deltapm02.dat")

	lt19
		1. copied from lt18, revise in veq19 uarray=np.array([ux0,ux1,uy0,uy1]) to see if it improve iteration at x=0,y=1e-4 (deltap=-0.02)
		2. change ari0 in lt19 correspondingly
		3. in slide 47 found the reloading in py3 does not work, and modified veq.uarray=np.array([ux0,ux1,uy0,uy1])
			in lt19 works well
		4. lt19 runm012n1() duplicate quite closly the result of slide 30 bu not exactly
		use rl("20140204_bare_1supcell_deltapm02.dat")

	lt20
		1. copied from lt19 to reproduce slide 35 using runm03(), runm04(), runm05() to understand bnmnew1,bnmnew2, bnmnew3
			which uses vthetascan_use_x0dlast,vthetascan_use_x0dlast1,vthetascan_use_x0dlast2 with fsolv, findso1,findso2,
			found reproduced slide 35 of lt15_veq20140204_bare_1supcell_deltapm02_x0y1em4_8.py
		2.runm012n2 use findso2 shows error occurred in inversev1v2, so even though vscan error is reduced to zero,
			the error in inversev1v2 makes convergence fig.61 worse than slide 31, or slide 48
		3 So this means we need to use findso3, and find module inversev1v2 which uses findso3 too.
			this is found in lt15_3
		uses rl("20140204_bare_1supcell_deltapm02.dat")


	lt21	
		1. copied from lt15_3.py with revision veq.uarray=np.array([ux0,ux1,uy0,uy1]) to overcome reloading veq problem of py3.
		2. for x=0,y=1e-4 case with veq.uarray=np.array([ux0,ux1,uy0,uy1]), runm012() of lt21  generate error counting better 
			than lt20, but still with error in inversev1v2 even with findso3 (the )
		uses rl("20140204_bare_1supcell_deltapm02.dat")


	lt22
		1. copied from lt21 to revise to remove the changes of v0norm in iteration process, and keep it constant for 
			same xmax,ymax.
		2. result shows the v0norm does not influence the convergence, so returned to lt15.
		uses rl("20140204_bare_1supcell_deltapm02.dat")

	
	lt.23
		1. copied from lt15, revise to make runm012 works again
		2. found the error in lt15!! in iteration the initial trial trajectory
			should have been last xyd obtained from inversev1v2 in bnmnew3, not the last xyd0 from vthetascan in bnmnew3
		3. But this still does not solve divergence problem: run at x=5e-4,y=1e-4 still 
			on resonance (unstabe, mutiple lines on main frequency)
		4. good convergence at x=15mm, resonance at x=10
		uses rl("20140204_bare_1supcell_deltapm02.dat")


	lt8xm1y6_3_10_2020_1.py
		1. to remember how move into resonance, revise lt8xm1y6.py to lt8xm1y6_3_10_2020.py to adapt to new 
			vph file "iter2DModules.py", then revise to lt8xm1y6_3_10_2020_1.py to allow recording steps
			of how to move into resonance by steps bu changing veq.uarray and ari0 during iteration.

		use rl("nsls2sr_supercell_ch77_20150406_1_deltap0.dat")

	lt24.py
		1. copied from lt23.py which is already same as lt15.py. Now implement steps in lt8xm1y6_3_10_2020_1.py
			to slowly changing linear combination starting from (ux0,uy0)
		2. the steps are all put inside iterxnew() (which means iteration on xnew ), 
			but the steps  does not work for x=0,y=1e-4 case. So I need to try change xnew slowly from x=-15mm	
			in lt25.py next
		uses rl("20140204_bare_1supcell_deltapm02.dat")


	lt25.py
		1.copied from lt24.py revise scan runm10t0m25() to see if possible to go from x=-15mm to x=0
		2.But x=-15mm does not converge while x=15mm is when xmin=-12.8. So scan from -12mm to -7mm using runm012() and start to blow up again.
		3.Conclusion: it is difficult to use nvar=4 from x=0 to x=-7mm with delta=-0.02.
		4. decide to change to nvar=2 and scan , copy lt25 to lt26.
		uses rl("20140204_bare_1supcell_deltapm02.dat")

	lt26.py
		1. copied from lt25.py to scan using nvar=2
		2. scan from x=-12mm to -7 mm, at -7mm, iteration blow up, showing another resonance
		3. scan from x=0 to -5mm, at -5mm iteration blow up, so there is a resonance between(-7mm,-5mm)
			which is different from the resonance at x=0. At x=0, with nvar=2, it is convergent
			but at -5mm, even for nvar=2 it blow up.
		4. So I need to go back to lt11 case to understand why the resonance in lt11 case with
			"veqnsls2sr_supercell_ch77_20150406_1_deltap0_lt11" can be solved while
			the case here for "20140204_bare_1supcell_deltapm02.dat" is not
		uses rl("20140204_bare_1supcell_deltapm02.dat")

	🛑lt27.py
		1. copy from lt26, study case of lt11 with x=-1mm,y=6mm case given in  "madxFrequencyMap1.pptx" slide 91.
		2. copy veq20140204_bare_1supcell_deltapm02.py as veq27.py and revise.
		3. keep initial copy of lt27.py in lt27_0_20140204_bare_1supcell_deltapm02.py for the case 20140204_bare_1supcell_deltapm02
		4. keep initial copy of veq27.py as veq27_0_20140204_bare_1supcell_deltapm02.py3
		5. revise lt27.py and veq27.py to allow for later lattice change by changing lt27, without changing veq27.py
		6. important milestone: compare with lt1, all sidelines agree, main peak resonance points in lt27 is much lower
			than lt1_m1.py. So lt27 not only improved result, also allows for change of lattice and initial points,
			most importantly, lt27's iterxnew() allows a sequence of converging to improve precision.
		7. different comparison: difference of 7.1 and 7.2 tells how to change lattice and initial xmax,ymax, thetadiv.
			7.1 lt27_0_20140204_bare_1supcell_deltapm02_x0y1em4_compare_lt24.py 				iterxnew(xmax=0,ymax=1e-4)
			7.2 lt27_nsls2sr_supercell_ch77_20150406_1_compare_lt1_xm1y6.py, same as lt27.py	iterxnew(xmax=-0.8e-3,ymax=6e-3)
			7.3 lt27_nsls2sr_supercell_ch77_20150406_1_compare_lt5_x22y4.py						iterxnewlt5(xmax=22e-3,ymax=4e-3)
			7.4 lt27.py, same as  lt27_nsls2sr_supercell_ch77_20150406_1_compare_lt1_xm1y6.py	iterxnew(xmax=-0.8e-3,ymax=6e-3)
			7.5 lt27_cb2NSLS2CB65pm_cb0_1cell_deltaPp005.dat_compare_lt6_p005.py 				iterxnew(xmax=-1.5e-3,ymax=0.1e-3)
			7.6 lt27_cb2NSLS2CB65pm_cb0_1cell_my.dat_compare_lt14_y3em4.py	  runitersamepoint(thetadiv, xmax=-2e-3, ymax=3e-4
			7.7 lt27_20140204_bare_1supcell_compare_lt16.py					  runitersamepoint(thetadiv, xmax=-15e-3, ymax=1e-4, 
		8.Examples to be compared with: to be studied:
			8.1 lt24 of slide 64 of madxFrequencyMap2.pptx (see 7.1) (in henonheilesshort/offmnew), Done.
			8.2 lt1 in slide 5 of sqmxnsls/latticeresonance.pptx (see 7.2,7.4) (in nonlineardynamics/henonheiles/) Done.
			8.3 lt5 in slide 14 of sqmxnsls_1_3_2020/latticeresonance.pptx and slide 9 of madxFrequencyMap3 (see 7.3) Done.
			8.5 lt15_cb2deltaP0p005.py of slide 23 of madxFrequnecyMap2, use cb2NSLS2CB65pm_cb0_1cell_deltaPp005.dat 
				at Xmax=-1.5e-3,ymax=1e-4, and compare with lt6.py slide 25 of madxFrequnecyMap1
				lt6 (recovered as lt6_p005.py 3/18/2020)
				uses jfM.dat  specified 5233010 bytes, when slide 25 specify deltap=0.005, it means
				it is jfM_jfmadxicase5.dat, even though now it is same as jfM_jfmadxicase4.dat
				it is confirmed that cb2NSLS2CB65pm_cb0_1cell_deltaPp005.dat is same as jfM_jfmadxicase5_deltaP0p005.dat
				Done! (see 7.5)
			8.6 Lt14, cb2NSLS2CB65pm_cb0_1cell.lte slide 17 of madxFrequnecyMap2, and compre with slide 13 of madxFrequnecyMap3
				cb2NSLS2CB65pm_cb0_1cell_madx.dat
				Study at x=-0.002, y=3e-4 used with deltap=0
				done (see 7.6)
			8.7 lt16 slide 42 of madxFrequnecyMap2, rl("20140204_bare_1supcell.dat"), xmax,ymax=-0.015,1e-4
				done. (see 7.7)

	lt28.py
		1. copied from lt27.py to improve iterxnew in lt27_0_20140204_bare_1supcell_deltapm02_x0y1em4_compare_lt24.py
			so that the resonance at x=0,y=1e-4 can be found with higher order uy1, in addition to ux0,uy0.
		2. increase nvar from 2 to 3 with success, get convergence with linear combination ux0,uy0,uy1
		3. Found during iteration, there is a need to use uselastxyd=0 for vthatascan to get best result.
		4. when increase to nvar=4 with ux0,ux1,uy0,uy1, it still blow up, showing resonance instability
			of present linear combination optimization process.
		5. lt28_0 use runbnm with iterlimt=1 for each step, lt28_1, use runbnm with iterlimt=3, and 5 for nvar=2, and 3
			respectively. lt28.py use runbnm with iterlimt=3,5,5 for nvar=2,3,4 respectively, but at nvar=4, blowup.

	lt29.py
		1. Study 20140204_bare_1supcell_deltapm02, case of from x from -4mm to -6mm, y=1e-4 resonance as blow-up in lt26
		2. copied from lt28.py (with pattern set by lt27.py 
		3. change from nvar=2 to to nvar=3 at -4.7mm, uarrau=ux0,uy0,uy1 failed, but uarray=ux0,ux1,uy0 works.
		4. But when move to 4.7125e-3, iteration 10 blows up again: showing I need to change anew12 to avoid resonances.

	lt30.py 
		1. copy from lt29.py and revise according to "iterationFormulas.lyx" in "/Users/lihuayu/Dropbox/henonheilesshort/offmnew/theory"
		2. 	2.1 modify in runbnm: if ik==0: replace if ik>-1
			2.2 modify vthetascanPrecision: if usexp0dlast!=0, skip vthetascan, and let xyd0 take previous xyd
		3. First use lt27_20140204_bare_1supcell_compare_lt16.py as example lt30_nousexyd_20140204_bare_1supcell_compare_lt16.py
			get same result (both use usexyd=0)
		4. Change so that only the first iteration uses vthetascan, all iteration use previous xyd:
			lt30_usexyd_20140204_bare_1supcell_compare_lt16.py 
		5. Result shows tha use previous xyd to replace vthetascan is worse.

	thought about my notes "iterationFormulas.lyx": the blow up happend for a given linear combination when the denominator
	is small, so change the linear combination after this would not help. The only possible way is to use the Jordan matrix
	property to find best linear combination to make the numerator of resonance term zero. This will be more complicated
	process to be done later. For now, I have to passs this and study low power order scan and pass resonance already to
	find dynamic aperture.

	lt31.py
		1.copied from lt29 and continue to extend x to x=-4.733e-3 , and trying to understand resonance at arecord[-1]
		2.Found the main change is aphi2[-4,-2] increases while -4,-2 is on resonance.
		3. revise vph.anew12 to anew12 in lt31 and use anew12 instead of vph.anew12 to see if improve convergence.
		4 preserve lt31.py old version using vph.anew12 as lt31_0.py
		5. lt31.py weight method in anew12 does not improve convergence, it is worse than lt29.py so need to
			try use constraint in anew12 to force resonance term to zero.
		6. plotxnewvsiteration(),plotnuxnuyvsiteration(),plotphi2iter(),plotv1v2iter89(),
			and studyerrorat195() to study why blow up at iteration 9 for x=0,y=1e-4 case
			using iterxnew2(weightpower), determined that phi2 spread suddenly increases at	
			iteration 9, so the problem is still xyd0 error in vthetascan.
		7. during studyerrorat195(), realize the output from vph.vthetascan_use_x0dlast3
			is hard to regocnize, decide to go to lt33 to revise this with dict.

	lt32.py
		1. copied from lt31.py, modify anew12 in lt32 to constraint resonance term to zero, according to derivcation
			in "constrainIterationFormulas.lyx"
		2. the constrain for resonance term=0 method failed miserably. so returned to lt32.
			
	lt33.py
		1.copied from lt31 to revise vph.vthetascan_use_x0dlast3 in lt33 as
			vthetascan_use_x0dlast3 but with findso revised to study why the errors started in iteration 9.
		2. use  arecord=iterxnew2(0), plotv1v2iter89(arecord,8), studyvthetacan(arecord) found error source
		3. use studyiter9at195(arecord) find problem in vph.findso3 and the use findso3 and findsolbysacle3
			revised with fmin to replace fsolve, and removed the errors in iteration 9

	lt34.py
		1. copied from lt33 to revise so all vh.findso3 is replaced by findso3 in related to vthetascan lt34.py
			keep vph.findso3 in inversev1v2, waiting to be treated later
		2. use arecord=iterxnew2(0), plotv1v2iter89(arecord,8), studyvthetacan(arecord),studyiter9at195(arecord)
			to check if the errors is changed in iteration 8,9
		3. in lt34_1, replaced fsolve by fmin in vthetascan
		4. in lt34, replace fsolve by fmin also in inversev1v2

	lt35.py
		1. copy from lt34, modify findsolbysacle3 so that most time it use fsolve the way as lt33
			but when the error is large than a tolerance, switch to fmin as lt34.
		2. the result of more accurate xyd and xnew from xyd in arecord[4] leads to blow up in vthetascan
			in arecord[5] after changing nvar from 2 to 3 showing uy1 does not help after iteration, but v in arecord[4]
			may be useful as a better action-angle variable.
		3. Found fsolve is much more faster and precise than fmin. and conclude that the blowup is due
			to resonance at origin for -2% delta for the case of x=-0, y=1e-4. So only change Jordan SimpleNamespace
			may solve this resonance problem.
		3. Since vthetascan may not have solution, it may be better to skip vthetascan after iteration 2.

	lt36.py
		1. copy from lt35, revise to skip vthetascan after first iteration for a new Jordan space introduced.
		2. result is a faster and does not worseining the blowup for x=0,y=1e-4, -2% case.

	lt37.py
		1.copy from lt36 revise to use nvfor each nvar=2 scan such that for each iteration it always start from vthetascan
			mainly because I found that it is important to use vthetascan too after iteration 0 to get this, i.e.,
			after using xydlast as previous xtrial, when move to next initial x with new value, with ik=0, still use vthetascan to 
			get xyd0 as first solution for that point.
		2. It is found that if use xnew from previous point, the result strongly dependent on scan path
			there is resonance peak at -5mm to -8mm, to get this result I need to run from -4 to -5mm, then from -12mm
			to -8mm
		3. Found use xnew0 for each point once only, get result similar to iterate 3 times using xnew0 first time then
		 	follow by iteration using xnew of previous iteration twice. So for fast scan, each point has only 1 point
			using xnew0.
		4. tried increase ymax from 1e-4 to 1e-3, and get blow up almost every where. Conclusion: 4nux+2nuy=1 resonance
			at-2% for 20140204_bare_1supcell_deltapm02.dat has lost precision, cannot work well, it is dominated by noise.
			So I need way to shift tune to exactly on reosnance.

	lte2deltapmadx2fort182mapMdat_detune.py
		1. revise lte2deltapmadx2fort182mapMdat.py to change tune to move near resonance dat file into exactly on resonance.
		
		 
	lt38.py, veq38.py
		1. copy lt37 to lt38, veq33 to veq38, revise to move map so 4mux+2muy=1 exactly, and revise BK matrix to take 
			into account of detunning.
		2. create lte2deltapmadx2fort182mapMdat_detune.py by revising lte2deltapmadx2fort182mapMdat.py to change tune.
		3. created 20140204_bare_1supcell_deltapm02_detune.dat which use same lte file as 20140204_bare_1supcell_deltapm02.dat
			but which is detunes so that 4nux+2nuy=1 is exactly satisfied, i.e., exactly on resonance
		4. milestone succsess: overcome resonace at origin.
	
	🛑lt39.py
		1. copied from lt38.py, revise to simplify and so speed up scan runbnm and runm012_usexnew0 from scan in one line to
			scan over x,y plane.
		2. success compare with frequency diagram on reosnance
		3. milestone
		4. used veq40 with imported lte2deltaPmadx2fort182mapscan_detune.py in veq40 so no need for *dat file.
		5. move anew12,findso3, findsolbysacle3, vthetascan_use_x0dlast3,inversev1v2 into iterModules to replace iter2DModules as vph

	🛑lt40.py 
		1. copy from lt39, revise veq38 used in lt39 by veq40 where the vphi calculation by tracking one turn
			is to be replaced by Jordan matrix tau square matrix  to speed up calculation (comment of 10/20/2020: not in veq40, eventually in future.).
		2. remove calculatiob related to inversev1v2, including "inversevinput,inversev1v2msg" in iorecordlist
		3. bnmoutlist, iorecordlist, flucbnmlist are revised to remove v12, xyd xnew etc.
		4. move findso3, findsolbysacle3, vthetascan_use_x0dlast3,inversev1v2 into iterModules to replace iter2DModules as vph
		5. clean up remove development modules: plotarecord,iterxnew,iterxnew2,studyiter4at255,studyiter4,optimizeinversev1v2
		6. Found at deltap=-2.5%, apperture is much smaller than frequency map, ux0,uy0 is not accurate, need renew xnew
			iteration

	lte2deltaPmadx2fort182mapscan_detune.py
		1. copied from jfdelta4.py as map generation by dpmap
		2. then revise and combine with lte2deltapmadx2fort182mapMdat.py to generate fluctuation map
			with p scan directly without saving jfM.dat file.
		3. Then import into veq40 and lt39 and lt40 to allow  to scan xy and  very deltap without using *dat files.

	lt41.py
		1.copy from lt39, revise and add back some module of lt38 to study errors at deltap=-2.5% case of 20140204_bare_1supcell.lte
		2. use runm012() to show iterate with xnew for nvar=2 does not reduce fluctuation
		3. reintroduce iterxyd back from lt9 to renew xyd and bnm12 instead of renew linear combination by xnew
			mainky because I found the resonance at origin caused passed problem and gave me a wrong impression
			that KAM iteration has to be done only after xnew iteration is done and seemed to be hard. Now lt40 has shown
			xnew iteration is reproducible after resonance problem at origin is solved by detuning to exact resonance.
		4. But after implement iteration of xyd as KAM method in runiterxyd(), it is seen for nvar =2 and without
			improved xnew iteration, the actiona-angle variable ux0,uy0 is not good enough for KAM iteration.
			so I decide to copy lt41 as lt42 to duplicate lt9_differ_only_by_plot_in_iter2DModules.py

	lt42.py
		1. copy lt41 as lt42 to duplicate lt9_x22mmxnew0.py to modify runiterxyd() into
			runlt9()
		2. generate variable groups Zpar, uvar to replace all global variables in veq42
		3. change findso3,findsolbysacle3, vthetascan_use_x0dlast3,inversev1v2 in iter2DModules39.py to iterModules42.py
			to allow transfer information into these module by Zpar and uvar. Remove vthetascan() in  iterModules42.py
		4. runlt9() agrees with lt9_x22mmxnew0.py
		5. Since variable transfer changed, the plots in shortarecord and showarecord are updated to fit changes.

	lt43.py
		1. copy from lt42.py to revise so that all setup of ltefilename, thetadiv, etc parameters are inside a module
			so that for each different case parameters can be specified in a small module.

	lt44.py
		1. copy from lt43.py, copy veq42.py as veq44.py, cp lte2deltaPmadx2fort182mapscan_detune.py as lte2tpsa2map.py
		2. modify in dpmap of lte2tpsa2map.py adding the following to replace tpsa by Yoshi's new tpsa
        	tmp=rl('nsls2sr_supercell_ch77_20160525_nslice20_nv4_norder7_delta0.pkl')
        	mftpsa=[tmp['x'],tmp['px'],tmp['py'],tmp['x']]#neglect delta and ct
		3. use lt27_7_2 in lt44 to check figure 126. found poor result.
		4. fixed problem 2,3, by using tpsa to calculate the twiss parameters
		5. So now the compilor for Yoshi's tpsa replaced madx, and start from lte file, we can run to get figure126.
		6. According to Yoshi, the x,xp,y,yp are replaced by x,px,y,py. The notation are mixed may be confusing
			in vphi, and xtracking, but the revision correctly represent the fact that elegant uses xp which is
			not the canonical variable.

	🛑lt45.py,veq45.py and lte2tpsa2map45.py
		1. copied from lt44.py, veq44.py and lte2tpsa2map44.py as 45. revision to implement tracy tracking
			so it would be consistent with Yoshi's tpsa. Also implemented usecode['tpsacode']='tracy','madx','yuetpsa'
		2. lt45.py lt27_7_2_1(xmax=-4e-3,ymax=6e-3) now can specify lte file, use tpsa 'tracy' and tracking 'tracy'
			usecode['use_existing_tpsa']=1, etc.
		3. Add options in lt27_7_1
		       	veq.TRACKING_CODE='ELEGANT'#'Tracy'
        		lte2tpsa.dmuytol=0.005
		4. duplicated 6 cases in lt27
		5. duplicated lt31,lt32,lt35,lt36,lt37,lt38,lt39,lt40

	lt45.py
		1. runm02 study why at deltap=-0.025 DA plot is poor. Foud even for deltap=-0.02 where DA plot is good for 2 variable,
			the change to 4 variable leads to blow up: a major understanding: the increase of nvar from 2 to 4, actually also increased
			the number of high order terms in jordan vectors, so that althogh the number of parapeters increased from 2 to 4 so we can eliminate
			2 more noise lines, the number of resonant lines increased even faster to 4 to 5 terms larger than 0.04 in bnm2 as seen from slide 40 of 
			madxfrequencyMap4, as result the xnew has larger noise.
		2. Thus increase nvar to find xnew linear combination fails if close to resonance line at the origin.
		3. lt45.lt41runm012 duplicate result of lt41.runm012, shows when xmax increased to -18mm for nvar=2, the fluctuation increased to >1 and stopped
			the validity of perturbation theory.
		4. The result of 3 here indicate to extend perturbation theory range we may need to use bnm iteration iterxyd to improve DA calculation.
		5. Based on 4. lt45.lt41runiterxyd iterate on xyd (which is also iterate on bnm). 
		6. Found lt41.runiterxyd has error, but even though there is error, it establishes that since nvar=2, the fluctuation at x=-18mm is so large, the 
			iterxyd does not reduce fluctuation very much when near DA.
		7. found lt42runlt9wrong(xmax=22e-3,ymax=4e-3), correct as lt42runlt9Correction(xmax=22e-3,ymax=4e-3)
		8. duplicate lt41runm012,lt41runiterxydCorrected(xmax=-12e-3,ylim=0.1,ymax=1e-4),lt42runlt9Correction(xmax=22e-3,ymax=4e-3)

	lt45.r1nvar4xnew1iterxyd2(xmax=-12e-3,ymax=4e-3,thetadiv=20,cutoff=10,iter=(3,3))
		1. combine iteration of xnew with iteration of xyd orbit. first iterate xnew, then iterate xyd.
			Found error in vph.plotvnv0v1 when plot he spectrum of xyd 
			found 10*nux+nuy=2.2131521312710682, while -10*nux+nuy= -2.050812078186173. Thus from 10nux to -10 nux get a wrong tune
			This is due to the error which comes from finite thetadiv, because it does not allow for frequncy higher than 10, relted Nyquist 
			criterion, aliasing.
		2. Based on sampling theorem, implement lattice['Couchylimit']['aliasingCutoff']=cutoff, found
			example for xmax=-12e-3,ymax=4e-3,thetadiv=20,cutoff=10, iteration is convergent, but for cutoff=9, blowup because
			the main resonance line is also cutoff at 9, and the result is wrong. 
		3. Conclusion: we need sufficient thetadiv so that we can use sufficient high cutoff to include resonance line and at the same time
			remove aliasing distortion. So for the choice of cutoff is found by experimenting.

	lt45. r2nvar4xnew1iterxyd2,r3nvar4xnew1iterxyd2,r4nvar4xnew1iterxyd2
		1.r2nvar4xnew1iterxyd2 use nvar=2: at xmax=-12.1mm, sensitive so at -12.2mm blowup. fluctuation is 0.63 at xmax=-18mm blowup.
		2.r3nvar4xnew1iterxyd2 increase to nvar=4, start from -5mm cannot pass xmax=-8e-3 because ymax is fixed at 4e-3
		3.r3nvar4xnew1iterxyd2 nvar=4, allow ymax changes using xnewiterlistwith ymax in list. use ymax=1e-4, 
			starting from xmax=-11mm, reached xmax=-24mm. Success but also problem: vonvergent KAM invariant beyond DA when tracking lost.
		4. r4nvar4xnewMiterxydN, niter=3, scan reached xmax=-24mm, but problem again: convergent at -25mm where traking lost particle
		5. r4nvar4xnewMiterxydN, introduce vb, plotvb(arecord), vbaction(i) to check iteration convergence using Cauchy criterion
		6. r4nvar4xnewMiterxydN, when niter=8, blowup at x=-20mm

	lt45.testiterxnewxyd,testiterxnewxyd1,testiterxnewxyd2
		1. iterxnewxyd to iterate both xnew and xyd in one iteration, test in testiterxnewxyd, then in testiterxnewxyd1
			found in testiterxnewxyd1, niter=8, iteration at x=-20mm, delta xyd reaches a minimum within 8 iteration
			at x=-23mm, minimum at iter 3 and then rises much.But it is convergent at x=-25mm,-26mm when traking particel lost.
			so need to increase iteration to 12 in testiterxnewxyd2. So when niter=8, it may have convergence even particel lost.

		2.testiterxnewxyd2, increase to niter=12, blowup at x=-23mm, 
		3.introduce deltaxyd(arecord,k), and choosepreviousxyd(arecord) in testiterxnewxyd2 to test convergence progress in every step
			and when blowup at an xmax, find previous best xnew,xyd pair to go to next xmax.
		4. testiterxnewxyd2 with try and error handling,passes x=-23mm blowup, but problem again: iteration convergent when particle lost.
			at x=-25mm

	lt45. accumulate(arecord),interploation_xyd(xyd,thetadiv1,thetadiv2)
		1. develop a function for a gven arecord, continue to iterate at the same xmax, or proceed to next xmax.
		2. add interploation_xyd(xyd,thetadiv1,thetadiv2) to increase thetadiv for xyd trajectory
		3. develop iterpolateThetadiv(arecordi, thetadiv2=40, cutoff2=20) to change arecord's thetadiv for iteration on larger thetadiv.

	lt45. develope one turn tpsa function as compared with exact tracking for one turn map in vphi
		1. veq.oneturntpsamap(x0,xp0,y0,yp0,oneturntpsa,code='yuetpsa'), this needs a tpsa: oneturntpsa as input
		2. the tpsa is generated by getoneturntpsa(arecordi,code='yuetpsa') from specified arecordi, and use its ltefilename,deltap,nv,norder,usecode in arecordi
		3. testonturnmap() to test the accuracy of the function from x0,xp0,y0,yp0 to x1,xp1,y1,yp1
		4. testaccumulatetpsa() to test accumulate using tpsa to continue calculation instead of tracking, but result diverges if start from tracking record/
		5. runtestiterxnewxyd2tpsa() to test using tpsa to start calculation from -18mm, but result is divergent: may be -18mm is too large?
		6. init_arecord() to start from a speccified point only once using runbnm to generate 
			corrected error in tpsa part in veq.vphi about shifting origin to fixed point, and removed 1 micron in elegant's ele file.

	lt45.py
		1.ar=runtestiterxnewxyd2tpsa(trackcode=‘tpsa') success in using tpsa as tracking. compared with trackcode='ELEGANT"
		2.ar=testinit_arecord_accumulate(trackcode='tpsa’) from -2mm to -26mm, but blowup using 'ELEGANT' with iteration
		3. regognize noise problem at -2mm with 'ELEGANT' small amplitude noise? to be avoided by using 'tpsa' as tracking?

	lt45.py
		Increase thetadiv with test_change_thetadiv_accumulate_by_tpsa(oneturntpsa='tpsa')

	lt45.py
		1.test increase ymax to 4e-3 with test_change_thetadiv_accumulate_by_tpsa, blowup at x=-6mm, forced to study r3nvar4xnew1iterxyd2 nvar=4
		2.testinit_arecord_accumulate, scan xmax for ymax=4mm cannot pass through xmax=-6mm
		3.both r4nvar4xnewMiterxydN and testinit_arecord_accumulate can start from xmax=-10.5mm, but cannot start from -12.5mm
		4.testinit_arecord_accumulate_iterxnewxyd shows, start from -10.5mm each point iterate 12 times cannot pass xmax=-15mm
		5.testiteration_after_r4nvar4xnewMiterxydN start from -10.5mm with iter=2, can get to xmax=-19mm, but with niter=12, can only reach -16mm
		6.after Use niter=2 with testinit_arecord_accumulate_iterxnewxyd with iter=2, then test_xmax_iteration_after_testinit_arecord_accumulate_iterxnewxyd 
			can iterate with iter=12 blow up at -17mm, just alittle better than 5.
		7.after testinit_arecord_accumulate with iter=2 and ymax=1e-4, t
		8. extend nvar=6 using init_arecord_nvar6 at xmax=-10.5mm, then use test_xmax_iteration_after_testinit_arecord_accumulate_iterxnewxyd
			to scan with iter=12 can reach also xmax=-16mm, but not better nvar=4.
		9. Use result of 8. at xmax=-16mm, y=4mm ar[13], increase to -16.5mm can reach -12, but at -16.6mm, can only reach ln(dxyd)=-11
	
	lt46.py
		1. copy from lt45, revise to allow averaging xyd0 for starting trajectory iteration.
		2. averaging leads to better initial convergence and reacjed x=-18mm, but continued iteration leads to error.
		3. Use test_after_accumulate_use_iterxyd() to study using KAM iteration by iterate on xyd only to continue from2. but failed.
		4. introduce Hao Yue's new tpsa with feature of PtTPSA.save, PyTPSA.load, and PyTPSA.inverse_map, incooperated with Yoshi's 
			tpsa lattice generation. 
		5. include trackcode='ELEGANT',oneturntpsa='tpsa',use_existing_tpsa=1 into 
			test_xmax_iteration_after_testinit_arecord_accumulate_iterxnewxyd1, 
			init_arecord_nvar6 
			init_arecord
		6. test_use_arecordi_to_get_map_then_iterate_on _ar(ar,xmax=-16e-3), can continue iteration from ar
		7. mystery: expierence strange good result after implementHaoyue's tpsa, but second day cannot reproduce good result.
		8. reconfirmed that nvar=6 does not improve convergence after implement new tpsa.

	lt47.py
		1.copy from and modify lt46, and modify iterModules42 to use Haoyue's inverse function.
		2.introduce init_tpsa, just to generate bnmnew3input,lattice,oneturntpsa,w,winv as initialization
		3.runxnew0, start_from_xnew0_iterate_on_xnew_and_xyd uses runxnew0 and accumulate to iterate at one point of xmax.
		4.t6estinit_arecord_accumulate_iterxnewxyd use init_tpsa and  start_from_xnew0_iterate_on_xnew_and_xyd
		5. start_from_previous_xnew_iterate_on_xnew_and_xyd continue from t6estinit_arecord_accumulate_iterxnewxyd use init_tpsa to iterate starting from previous good xnew.
		6.scanxmax_followed_by_individual_points combine t6estinit_arecord_accumulate_iterxnewxyd and start_from_previous_xnew_iterate_on_xnew_and_xyd

	lt48.py
		1. delete functions no longer used from lt47 , now named lt48.py sent folder offmnewtpsa compressed to Yoshi.
		2. allow tpsa norder=7 and norder_jordan=3, get best result, sent to Yoshi
		3. move several offmnew..back folders into junk.dir
		4. study DA, cannot find difference between -22mm and -23mm for y=4mm			 
		5. compare one turn tracking square matrix with tpsa one turn square matrix, difference too small.
		6. change thetadiv at -22mm from 20 to 40, sure only at -22mm xyd disagrees with tracking with npass=800, 
			but agree at npass=6400, and particle lost at npass=6400, showing DA is within -22mm.

	lt49.py
		1. Found tpsa very time consumming
		2. replace in vphi the one turn tpsa by one turn matrix map, the speed increased by a factor of 12.
		3. The white spots in yoshi's fluctuation plot on 1/4/2021 is due to log(dx)=lon(0), is fixed in plotxydconvergence, and told Yoshi
			to correct it in scanmin.
		4. in anew12, introduce cutoff to match cutoff to match the cutoff in bnmphiCouchyCutoffWithIntrokam
			this reduce 0.05sec delay in anew12 to 0.01sec=10ms.

	lt50.py
		1. copy from lt49, introduce "fsolvebyscaleHomeMade", then "fsolvebyscaleHomeMadeblock", "findsoblock" into "inversev1v2"
			to replace "inversev1v2_old"
		2. "fsolvebyscaleHomeMade" reduce one "inversev1v2_old" run from 0.2sec to0.1sec.
		3. "fsolvebyscaleHomeMadeblock", "findsoblock" reduce to 0.03sec=30ms of "inversev1v2" so home made fsolve makes fsolve time 
			neglegible. 
		4. study relation between nsosteplim and error bar of fvec, and introduce fevtol in findsoparamters important, discussed in slide 16,17
		5. Time spent in bnmnew3:

			vphi  									6.5ms
			bnmphiCouchyCutoffWithIntrokam, 		4ms
			Inversev12 								7ms
			anew12									12ms

			Sum					6.5+4+7+12=30ms= 	0.03
			13 iteration of inversev12				0.4s
			Use winv for vthetascan 				0.08s
			total for x=-21mm						0.48sec.
		6. overcome blow up problem  at -7mm,-8mm, introduce in runiterxnewxyd criterion of termination of iteration based on 2 increases in deltaxydlist
			after reaching minimum.
		7. introduce  vinv_matrix into vthetascan to use wvinv in matrix from and calculate zs by zcolarray, reduce vthetascan time from 86ms to 11ms.
			result is 25 point scan reduced from 11sec to 10.8sec.
		8. use slide 21 of madxFrequencyMap6 to decide use first deltaxyd>0 as criterion for stopping iteration based on one point data only.
		9. When increase to bnmnew3input["findsoparamters"]["nsosteplim"] = 3 to improve precision of 0.015 errors in fvec , 
			there is a break problem when using "fsolvebyscaleHomeMade". problem solved as two mistakes given in slides 23-24 of
			"madxFrequencyMap6.pptx".
		10. study how to reduce fvec error bar. With nsosteplim=2, set ik<2 in “fsolvebyscaleHomeMadeblock”. while (
			maxdx > xtol and maxdx > fevtol and ik < 2),Get precision fvec error 8e-5, within 1.36sec.  

		11. control when stop iteration when beyond DA:  if deltaxydlist[-1] > 0: return arecord
		12. make plots: ferrorminlist = plotferrormin(ar), maxbmnlogav = plotbnm(ar)

	lt51.py	
		1. copy from lt50.py, change useBlockfsolve in vthetascan_use_x0dlast3 in iterModules50.py, so initial xyd0 is calculated from winvmtx
			instead of tpsa, and in wwJwinvf of veq50, use wwJwinv = wmtx, wJmtx, w0mtx, winvmtx, w, wJ, norder_winv, powerindex_winv
			instead of wwJwinv = wmtx, wJmtx, w0, winv, w, wJ so tpsa part is replaced my matrix.
		2. introduce norder_winv=5 and powerindex_winv(5) to save tpsa time on calculation of winv: norder_winv=7 order takes too long 10sec, )
			while norder_winv=3 has large error in inverse function.
		3. time spent in bnmnew3 is reduced one by one as
			vscan								   13.5ms

			vphi  									6.5ms
			bnmphiCouchyCutoffWithIntrokam, 		7.5ms-->0.5ms
			Inversev12 								16ms
			anew12									15ms-->0.1ms

			Sum				6.5+7.5+16+15=45ms= 	0.045s
			13 iteration without vscan				0.585s
			vthetascan 								0.013s
			total in bnmnew3 for x=-21mm			0.6sec.
			dpmap									0.72sec
			total for x=-21mm						1.32sec.

	lt52.py
		1. copied from lt51.py and introduce zcolnew.so, mysum.so lineareq.so f2py files to speed up
		2. develop inverse matrix invmatrix.f and lineareq.f in "fsolvebyscaleHomeMadeblock" reduce 3.2ms to 0.14ms
		3. zcolnew reduce Zs time from 1.8ms to 0.5ms for 7'th order, from 0.4ms to 0.16ms for 3rd order
		4. many small improve in invdvdx ("fsolvebyscaleHomeMadeblock") to speed up.
		5. improve "inversefftnew2"
		6. apply mysum.f in "bnmphiCauchyCutoffWithIntrokam" and 9 of "invdvdx"
		7. replace Zsd in bnm 4 by Zsdx equivalen but available already in inversev1v2.replace in Zsd in Fmatrix by Zsdx, Vmtx and wmtx to avoid using Zsd and 
			thus save time.
		8. Failed attempt 1: in bnmnew3 input and output introduce Zsdx associated with xdy0 or xydlast trying to use previous Zsd x as the new Zsdx generated
			from previous xyd. But failed because in our iteration, we average xyd to remove deviation from a true action angle variable. So the new
			trial xyd is different from previous xyd. Hence the previous Zsdx cannot be used.
		9. Failed attempt 2: to use the Zs0 in oneturnmatrixmap in vphi for the calculation of zs0 = Zszaray(xy0, Zpar). It failed because
			the oneturnmap is using x,xp,y,yp relatve to origin, while the xyd in vphi is relative to fixed point, so the Zs0 from one turnmap
			cannot be used.
		10. failed attempt to use mylog.f to replace python log in vphi 6. it spent 0.11ms longer than python's log of 0.1ms.
		11. Add nuxvzx to plot tune vz. x, found using nloss the cases for lost tracking particles, and non-convergent point in sqmx calculation
			roughly agree.
		12. modified way of send cutoff to functions, test norder=5, thetadiv=12 and cutoff=5.
		13. calculated the time of iteration, tpsa and sqmtrix construction for norder=5,7, thetadiv=12,20.

	lt53.py	
		1. copied from lt52.py, speed up is done, start new direction to study how DA related to convergence rate, with relation to cutoff.
		2. module test_change_thetadiv_accumulate_by_tpsa2, to change nth from ar obtained from xnew0 iteration at nth=12, study divergecne by increasing nth.
		3. tried to find when nth=18 blowup the growingmode pattern does not show a specific resonance in slide 1-7, did not see any clear relation with mode number.
		4. slide 8 shows even though elegant and tpsa one turn calculation differ but the behavior follow similar pattern
		5. use scan_around_DA to study the effect of increased nth, using nuxvzx to see tune.			
			scanx_starting_from_xnew0 is used in scan_around_DA, not used anymore, equivalent to lt53.t6estinit_arecord_accumulate_iterxnewxyd
		6. realized the importance to compare with number of survival turns.
		7. start tracking to see number of survival turn number by survival_turn,turn_number_averaging, and turn_number_averaging_at_x
		8. see the random, chaotic turn number-x relation, use statistical average to plot it.
		9. keep tracking result in 'nturn_record, which is large file but cover only narrow x range, "nturn_record_2" use 20 point around every 0.2mm
			so the data file is smaller, and collected more systematically in slide 16.
			survival_turn
			turn_number_averaging
			turn_number_averaging_at_x
		10. scan_around_DA() 
			(initial called scan_cutoff_around_DA) scan x with fixed nth, cannot give correct impression.
		11. scan2_ar2_theta_around_DA 
			(initial called scan_ar2_cutoff_around_DA) scan n_theta for fixed x, slide 18-25, found important result:
			number of survival turns is related to range of nth where iteration blow up around a band of n_theta, i.e., not a single thetadiv, but a contineous 
			rnge of thetadive where blowup happend is a indication of lose of particle.
		12. plot3Dtheta_vs_xydconvergence
			plot1Dtheta_vs_xydconvergence_from_3Ddata
			both are used to plot the result of theta scan

		13.	scan3_ar2_at_specified_nth12_around_DA scan with only nth1, nth2
			used to understand how to get DA from only two nth1,nth2 scan for fixed x, later evolves into scantheta_around_DA
			both eveolved into scanDA, searching for x 

	
	lt54.py
		1.copy from lt53.py, use lt53 as iteration module, and remove most iteration module from lt54, develop applications. using 
		2.study relation of number of turns to n_theta (i.e., thetadiv)
		3.scanDA is used to first scan xmax from -1mm to -28mm every 1mm starting from xnew0 to generate ar, 
			lt53.t6estinit_arecord_accumulate_iterxnewxyd
			plot3Dxydconvergence
		4. then use chosen ar[i] for first xmax where ar[i] diverges, then find next x where iteration converges, 			
		5. start from the first divergent xmax, scantheta_around_DA generate ar2 using ar[staring_index], while staring_index is found by 
			determine_starting_index_for_scantheta()
			scantheta_around_DA,
			determine_starting_index_for_scantheta()
		6. the criterion for divergence and convergence is expressed in two places in scanDA:
			5a:in scanmin as 'diverge', which is used in scanDA() to determine where a convergent ar[i] 
				(ar is inthe scan of xmax starting from xnew0) is to be used as the starting point of the scan of n_theta for  given xmax.
			5b:in determine_DA(x_conv_div_list) in scanDA, after the ar2 scan (ar2 is the scan of n_theta for intermediate x in searching for DA),
				the scan ar2 gives x_conv_div_list as a list of x and associated divergent (1) or convergent iteration (0). Then use determine_DA
				to determine whther both nth1,nth2 give divergence, if both diverge then the x is determned as divergent
			scanmin,
			determine_DA
		7. determine_next_iterate_step_xmax 
				is used to determine the step size reduce by half, and determine the search direction and whether search
				reached desired resolution.then reduce step size to find next diverging x until DA
				is found within specified resolution.

		8. plot_scanmin, and plotDA 
			generate convergence rate vs.x for ar scan and plot DA as two vertical lines as DA boundary 
			between convergence and divergence

		9. nuxvzx(ar) plot amplitude dependent tune, compare with tracking
		10.example1() show scanDA,nuxvzx:: how to find DA using only nth1, nth2 and few step of x searches. How to plot convergenc vs.x, and tune vs.
		11.example2() show scan2_ar2_theta_around_DA,scanmin,: how to plot convergence vs. iteration and nth for a fixed x
		12.plot_scan_x_theta_record()
			used to confirm the relation of number of survival turns with the appearance of band of nth's divergence points
			develop averaging over data to compare with "nturn_record_2", "nturn_record"
	
	lt55.py, iterModule55.py, iterlt53.py, veq55.py
		1. copied from lt54, remove functions which are used in development, and no longer used. revise to simplify useful functions.
		2. iterlt53.py copied from lt53.py remove some old fuctions, and simplify crucial function as as
			init_tpsa, start_from_xnew0_iterate_on_xnew_and_xyd, t6_init_iterxnewxyd (name changed),
			change_thetadiv2_in_findsoparamters,
			most importanty, iterxnewxyd, added iterate_condition.
		3. add oneturntpsa="tpsa",  #'ELEGANT',# to allow using elegant for one turn map
			and realized, when using elegant for exact one turn map, the iteration has larger noise than tpsa
			this was noticed before, but forgotten. Important to remember.
		4 iterModule55.py copied from iterModule52.py, remove old functions, simplify crucial functions like inversev1v2
		5. simpify inversev1v2:
			remove else part of "if useBlockfsolve", i.e., those used findso3, findso4 slow fsolve function. 
			remove "if len(inversevinputset) == 2" and related part, only use  inversevinput.
			revise relevant part in bnnew3 about inversevinputset
		6. simplify vthetascan_use_x0dlast3
			remove "if len(vthetascanset) == 2" and related part, only use  vthetascaninput.
			remove else part of "if useBlockfsolve", i.e., those used findso3, findso4 slow fsolve function. 
			revise relevant part in bnnew3 about vthetascanset
		7. in iterModule55.py removed finso4, finso5, findso3, etc. old functions from iterModule52.py
		8. veq55.py copied from veq52.py, remove old functions
		9. Test y=8e-3, and DA find is wrong. debug led to revise plotxydconvergence in test_change_thetadiv_accumulate_by_tpsa2
			as plot1Dtheta_vs_xydconvergence_from_3Ddata.
		10. scanxy() to scan xy plane for DA
		11. change offmnewmatrix into offm-short-newmatrix to delete large unused files. Put non-essential files in a folder
		12. To change iteration_condition
				In iterate_condition
				if (
				deltaxydlist[-1] > 0
				or np.isnan(deltaxydlist[-1])
				or np.log(
				abs(arecord[-1]["iorecord"]["inversev1v2msg"]["outfindso"]["fval"])
				)
				> 0
				):
				iterate = 0

				In fsolvebyscaleHomeMadeblock
				msg["fval"] = np.max(
				vscale2[:, 0].tolist()
				+ vscale2[:, 1].tolist()
				+ vscale2[:, 2].tolist()
				+ vscale2[:, 3].tolist()
				) # np.sqrt(tmp1 + tmp2)

	lt56.py
		1. copied from lt55, chenge some parameters to study ntheta=4 to extend convergence range larger and reach resonance, then increase ntheta
		2. in exampl6, found it is possible to get convergence on resonance by starting from ntheta=4, then change to ntheta=17
		3. change Couchylimit to Cauchylimit in lt56.py, lt55.py, iterlt53.py, lt55tmp.py, lt55_cb2NSLS2CB65pm_cb0_1cell.py, iterModules55.py, 
		4. allow Cauchylimit cut to study if convergence can be extend to much higher ntheta than 17 for -19mm case.

	lt57.py
		1.copy from lt55.py and revise to allow scanxy to change element strengths without changing lte file.
			by introducing Yoshi's function modify_LTE_elem_properties in veq55.py











	





	

🍎publication   arXiv:1809.00097v1 [math-ph] 1 Sep 2018		
list of *dat files:
	1. 	20140204_bare_1supcell_deltapm02.dat is generated by lte2deltapmadx2fort182mapMdat.py		
	2.  20140204_bare_1supcell_deltapm02_detune.dat is generated by lte2deltapmadx2fort182mapMdat_detune.py, detuned such
		that 4nux+2nuy=1 is exactly satisfied.
	3.  jfM_jfmadxicase5_deltaP0p005.dat is generated by jfdelta1Pmadx1mapM.py with cb2NSLS2CB65pm_cb0_1cell.lte

learned things:

1. how to use dictionay experience example on 2/4/2020:
vtmp="aphi1,aphi2".split(',')
vtmp2={ vtmp[i]:list(map(eval,vtmp))[i] for i in range(2)}
(vtmp2[vtmp[0]]==vtmp1[0]).all()
exec(list(vtmp2.keys())[0]+'=vtmp2[list(vtmp2.keys())[0]]')

2. how to find creation date?
mdls -n kMDItemFSCreationDate lt15_cb2deltaP0p005.py

3. How to use namespace for dictionary
from types import SimpleNamespace
ns = SimpleNamespace(**L)
L = dict(q=0.1, s=1.0)
ns = SimpleNamespace(**L)
In [11]: ns
Out[11]: namespace(q=0.1, s=1.0)
In [12]: ns.q
Out[12]: 0.1
In [13]: for k, v in L.items():
    ...:     globals()[k] = v
    ...:

In [14]: q
Out[14]: 0.1

In [15]: s
Out[15]: 1.0

4.%pdb #this can turn on or turn off debug in ipython
	once in debug, with an error in a subroutine, it will get into the subroutine
	if in the subroutine, type q will quit the subroutine
	type %pdb again will turn debug off

5.collaps all cells:
	commands+k+0

6. visual code: split separate a new window screen:
	command+shift+n

7. when "select debuging configuration", the way to quit this is to select current python file.
8. debug warning signal:
        np.seterr(all='raise')
        try:
            den=Dx*Dy # 3rd term of Eq.3 of flatbeam
        except:
            import pdb; pdb.set_trace()

9. visual code split screen
	view--> command palette--> type at the top> split down 

10. find files with a specified string in all subdirectory
find . -name '*.py' -exec grep -i "def xtracking(x0"  {} \; -print

11. Find index of maximum in array
	i,j=np.unravel_index(absM.argmax(), absM.shape)

	if af = abs(M), where M is an array then the last 5 sorted indexes of M are:
	[ np.unravel_index(i, af.shape) for i in argsort(np.ravel(af))[-5:]]


12. example:
Hao, Yue
Wed 1/20/2021 6:38 PM
Could you check if the following works using your actual F array:

 

You can replace the part
alpharangeA = [
        [i, j]
        for i in range(-cutoff // 2, cutoff // 2)
        for j in range(-cutoff // 2, cutoff // 2)
        if [i, j] != [1, 0]
    ]
    for i in range(nvar):
        for j in range(nvar):
            for alpha in alpharangeA:
                A[i, j] = (
                    A[i, j]
                    + np.conjugate(F[i, alpha[0], alpha[1]]) * F[j, alpha[0], alpha[1]]
                )
with
    Fa=np.repeat(x[:,np.newaxis,:], nvar, axis=1)
    Fb=np.repeat(x[np.newaxis,:,:], nvar, axis=0)
    exclude=(Fa*Fb)[:,:, 1,0]
    A=np.sum(Fa*Fb, axis=(2,3))-exclude

The A matrix should be same before and after substitution.  Since I cannot check it, there may be small errors. I can fix if you find these two A are not same.
Yue

Yoshi,
    iCut = cutoff // 2
    Fcut = np.concatenate((F[:, :iCut, :], F[:, -iCut:, :]), axis=1)
    Fcut = np.concatenate((Fcut[:, :, :iCut], Fcut[:, :, -iCut:]), axis=2)
    Fa = np.repeat(Fcut[:,np.newaxis,:], nvar, axis=1)
    Fb = np.repeat(Fcut[np.newaxis,:,:], nvar, axis=0)
    FF = Fa.conj() * Fb
    exclude = FF[:,:, 1,0]
    A = np.sum(FF, axis=(2,3)) - exclude
	
13.example 2
Hao, Yue <haoy@frib.msu.edu>
Fri 1/22/2021 11:51 AM
Yes, you can replace the entire block by:
hl=np.arange(-cutoff//2, cutoff//2)
vl=np.arange(-cutoff//2, cutoff//2)
hh,vv=np.meshgrid(hl,vl)
freqfac=np.exp(1j*hh*om1+1j*vv*om2)-1
freqfac[-cutoff//2, cutoff//2]=1+0j
bnm1=aphi1/freqfac
bnm2=aphi2/freqfac
this leave the (0,0) dc amplitude unchanged from aphi to bnm
Again please check if this produce the same result first.
Yue

Yoshi,
    iCut = cutoff // 2
    rows = cols = np.arange(-iCut, iCut)
    inds = np.ix_(rows, cols)
    div_fac = np.exp(1j * (inds[0] * om1 + inds[1] * om2)) - 1
    bnm1cut = aphi1[inds] / div_fac
    bnm2cut = aphi2[inds] / div_fac
    bnm1 = np.zeros_like(aphi1)
    bnm2 = np.zeros_like(aphi2)
    bnm1[inds] = bnm1cut
    bnm2[inds] = bnm2cut
    bnm1[0,0] = 0j
    bnm2[0,0] = 0j

13. visual studio code 
	find definition stoped working solution: File--> add Folder to  workplace
 